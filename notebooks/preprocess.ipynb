{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning for BI Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP & IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define file paths based on your structure\n",
    "RAW_DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = '../data/processed'\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "print(f\"Data Source: {RAW_DATA_DIR}\")\n",
    "print(f\"Export Target: {PROCESSED_DATA_DIR}\")\n",
    "print(\"\\n‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 1: DATA INGESTION\n",
    "# ============================================\n",
    "\n",
    "# Helper function for flexible sheet detection\n",
    "def find_sheet(xls_file, possible_names):\n",
    "    \"\"\"Find sheet by trying multiple name variations\"\"\"\n",
    "    available = xls_file.sheet_names\n",
    "    for name in possible_names:\n",
    "        # Exact match\n",
    "        if name in available:\n",
    "            return name\n",
    "        # Case-insensitive match\n",
    "        for sheet in available:\n",
    "            if sheet.lower() == name.lower():\n",
    "                return sheet\n",
    "    # If not found, return first sheet as fallback\n",
    "    print(f\"   ‚ö†Ô∏è  None of {possible_names} found. Using first sheet: {available[0]}\")\n",
    "    return available[0]\n",
    "\n",
    "# --- Load Student Survey ---\n",
    "student_path = os.path.join(RAW_DATA_DIR, 'student_survey.xlsx')\n",
    "student_xls = pd.ExcelFile(student_path)\n",
    "\n",
    "print(f\"üìÇ Student Survey File: {student_path}\")\n",
    "print(f\"   Available sheets: {student_xls.sheet_names}\")\n",
    "\n",
    "# Find the data and codebook sheets\n",
    "data_sheet = find_sheet(student_xls, ['Data', 'data', 'DATA', 'responses', 'raw_data'])\n",
    "codebook_sheet = find_sheet(student_xls, ['Codebook', 'codebook', 'CODEBOOK', 'metadata', 'dictionary'])\n",
    "\n",
    "print(f\"   ‚úì Using data sheet: '{data_sheet}'\")\n",
    "print(f\"   ‚úì Using codebook sheet: '{codebook_sheet}'\")\n",
    "\n",
    "# Read the sheets\n",
    "df_student_raw = pd.read_excel(student_xls, sheet_name=data_sheet)\n",
    "df_student_meta = pd.read_excel(student_xls, sheet_name=codebook_sheet)\n",
    "\n",
    "print(f\"   Student data shape: {df_student_raw.shape}\")\n",
    "print(f\"   Student codebook shape: {df_student_meta.shape}\")\n",
    "\n",
    "# --- Load Teacher Burnout ---\n",
    "teacher_path = os.path.join(RAW_DATA_DIR, 'teacher_burnout.xlsx')\n",
    "teacher_xls = pd.ExcelFile(teacher_path)\n",
    "\n",
    "print(f\"\\nüìÇ Teacher Burnout File: {teacher_path}\")\n",
    "print(f\"   Available sheets: {teacher_xls.sheet_names}\")\n",
    "\n",
    "# Find the data and subtitles sheets\n",
    "data_sheet_t = find_sheet(teacher_xls, ['Data base', 'Data_base', 'Database', 'data', 'Data'])\n",
    "subtitle_sheet = find_sheet(teacher_xls, ['Subtitles (codes)', 'Subtitles', 'codes', 'Codes', 'legend'])\n",
    "\n",
    "print(f\"   ‚úì Using data sheet: '{data_sheet_t}'\")\n",
    "print(f\"   ‚úì Using subtitle sheet: '{subtitle_sheet}'\")\n",
    "\n",
    "# Read the sheets\n",
    "df_teacher_raw = pd.read_excel(teacher_xls, sheet_name=data_sheet_t)\n",
    "df_teacher_meta = pd.read_excel(teacher_xls, sheet_name=subtitle_sheet)\n",
    "\n",
    "print(f\"   Teacher data shape: {df_teacher_raw.shape}\")\n",
    "print(f\"   Teacher subtitles shape: {df_teacher_meta.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PHASE 1 COMPLETE - Data Ingestion Successful\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1.5 DATA STRUCTURE INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 1.5: DATA STRUCTURE INSPECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STUDENT SURVEY - CODEBOOK STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Columns: {df_student_meta.columns.tolist()}\")\n",
    "print(f\"Shape: {df_student_meta.shape}\")\n",
    "display(df_student_meta.head(15))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STUDENT SURVEY - RAW DATA STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Columns: {df_student_raw.columns.tolist()[:20]}...\")  # First 20 columns\n",
    "print(f\"Shape: {df_student_raw.shape}\")\n",
    "display(df_student_raw.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEACHER BURNOUT - SUBTITLES (CODES) STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Columns: {df_teacher_meta.columns.tolist()}\")\n",
    "print(f\"Shape: {df_teacher_meta.shape}\")\n",
    "display(df_teacher_meta.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEACHER BURNOUT - RAW DATA STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Columns: {df_teacher_raw.columns.tolist()}\")\n",
    "print(f\"Shape: {df_teacher_raw.shape}\")\n",
    "display(df_teacher_raw.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 2: MAPPING LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 2: MAPPING LOGIC\n",
    "# ============================================\n",
    "\n",
    "def parse_student_values(values_str):\n",
    "    \"\"\"\n",
    "    Parses student codebook 'values' column.\n",
    "    Example: '1: \"Full-time\", 2: \"Part-time\"' ‚Üí {1: 'Full-time', 2: 'Part-time'}\n",
    "    \"\"\"\n",
    "    if pd.isna(values_str) or values_str.strip() == '':\n",
    "        return {}\n",
    "    \n",
    "    mapping = {}\n",
    "    \n",
    "    # Split by comma (handles multiple value pairs)\n",
    "    pairs = values_str.split(',')\n",
    "    \n",
    "    for pair in pairs:\n",
    "        if ':' in pair:\n",
    "            try:\n",
    "                # Split by first colon only\n",
    "                code_part, label_part = pair.split(':', 1)\n",
    "                \n",
    "                # Clean up\n",
    "                code = code_part.strip()\n",
    "                label = label_part.strip().strip('\"').strip(\"'\")\n",
    "                \n",
    "                # Convert code to integer if possible\n",
    "                try:\n",
    "                    code = int(code)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                \n",
    "                mapping[code] = label\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not parse pair '{pair}': {e}\")\n",
    "                continue\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def create_student_mappings(codebook_df):\n",
    "    \"\"\"\n",
    "    Creates {column_name: {code: label}} dictionary from student codebook.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    \n",
    "    for _, row in codebook_df.iterrows():\n",
    "        col_name = row['name']\n",
    "        values_str = row['values']\n",
    "        \n",
    "        # Parse the values string\n",
    "        value_map = parse_student_values(values_str)\n",
    "        \n",
    "        if value_map:  # Only add if we got valid mappings\n",
    "            mappings[col_name] = value_map\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "\n",
    "def parse_teacher_subtitle(subtitle_text):\n",
    "    \"\"\"\n",
    "    Parses teacher subtitle format.\n",
    "    Example: \"Female (1) Male (2)\" ‚Üí {1: 'Female', 2: 'Male'}\n",
    "    Example: \"Single (1) Married or married (2) Divorced or separated (3) Widower (4)\"\n",
    "    \"\"\"\n",
    "    if pd.isna(subtitle_text) or subtitle_text.strip() == '':\n",
    "        return {}\n",
    "    \n",
    "    mapping = {}\n",
    "    \n",
    "    # Find all patterns like \"Label (number)\"\n",
    "    # This regex captures: word/phrase followed by (number)\n",
    "    pattern = r'([A-Za-z\\s\\-/]+)\\s*\\((\\d+)\\)'\n",
    "    matches = re.findall(pattern, subtitle_text)\n",
    "    \n",
    "    for label, code in matches:\n",
    "        label = label.strip()\n",
    "        code = int(code)\n",
    "        mapping[code] = label\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def create_teacher_mappings(subtitle_df):\n",
    "    \"\"\"\n",
    "    Creates {column_name: {code: label}} dictionary from teacher subtitles.\n",
    "    Note: The subtitle sheet has ONE ROW where each column contains the coding info.\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "    \n",
    "    # Get the first row (index 0) which contains all the subtitle information\n",
    "    subtitle_row = subtitle_df.iloc[0]\n",
    "    \n",
    "    for col_name in subtitle_df.columns:\n",
    "        subtitle_text = str(subtitle_row[col_name])\n",
    "        \n",
    "        # Parse the subtitle\n",
    "        value_map = parse_teacher_subtitle(subtitle_text)\n",
    "        \n",
    "        if value_map:  # Only add if we got valid mappings\n",
    "            mappings[col_name] = value_map\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "\n",
    "print(\"‚úì Custom parsing functions defined\")\n",
    "print(\"\\nTesting student parser:\")\n",
    "test_student = '1: \"Full-time\", 2: \"Part-time\"'\n",
    "print(f\"  Input: {test_student}\")\n",
    "print(f\"  Output: {parse_student_values(test_student)}\")\n",
    "\n",
    "print(\"\\nTesting teacher parser:\")\n",
    "test_teacher = \"Female (1) Male (2)\"\n",
    "print(f\"  Input: {test_teacher}\")\n",
    "print(f\"  Output: {parse_teacher_subtitle(test_teacher)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 3A: DECODE STUDENT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 3A: DECODE STUDENT DATA\n",
    "# ============================================\n",
    "\n",
    "# Create mappings from the codebook\n",
    "student_mappings = create_student_mappings(df_student_meta)\n",
    "\n",
    "print(f\"Created mappings for {len(student_mappings)} student columns\")\n",
    "print(f\"Sample columns with mappings: {list(student_mappings.keys())[:5]}\")\n",
    "\n",
    "# Create a copy to work on\n",
    "df_student_clean = df_student_raw.copy()\n",
    "\n",
    "# Apply mappings\n",
    "decoded_count = 0\n",
    "for col in df_student_clean.columns:\n",
    "    if col in student_mappings:\n",
    "        print(f\"  Decoding: {col}\")\n",
    "        # Map values, keep original if no match found\n",
    "        df_student_clean[col] = df_student_clean[col].map(student_mappings[col]).fillna(df_student_clean[col])\n",
    "        decoded_count += 1\n",
    "\n",
    "print(f\"\\n‚úì Decoded {decoded_count} columns in student data\")\n",
    "\n",
    "# Add metadata columns\n",
    "df_student_clean['User_Type'] = 'Student'\n",
    "df_student_clean['Dataset_Source'] = 'Global_Student_Survey_COVID19'\n",
    "\n",
    "print(\"\\n--- STUDENT DATA SAMPLE (DECODED) ---\")\n",
    "# Show some key columns if they exist\n",
    "sample_cols = [col for col in ['Q3', 'Q4', 'Q5', 'Q8', 'User_Type'] if col in df_student_clean.columns]\n",
    "if sample_cols:\n",
    "    display(df_student_clean[sample_cols].head(5))\n",
    "else:\n",
    "    display(df_student_clean.iloc[:, :5].head(5))\n",
    "\n",
    "print(f\"\\nFinal Student Data Shape: {df_student_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3B - Decode Teacher Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 3B: DECODE TEACHER DATA\n",
    "# ============================================\n",
    "\n",
    "# Create mappings from the subtitles\n",
    "teacher_mappings = create_teacher_mappings(df_teacher_meta)\n",
    "\n",
    "print(f\"Created mappings for {len(teacher_mappings)} teacher columns\")\n",
    "print(f\"Columns with mappings: {list(teacher_mappings.keys())}\")\n",
    "\n",
    "# Show one example mapping\n",
    "if teacher_mappings:\n",
    "    first_key = list(teacher_mappings.keys())[0]\n",
    "    print(f\"\\nExample - {first_key} mapping: {teacher_mappings[first_key]}\")\n",
    "\n",
    "# Create a copy to work on\n",
    "df_teacher_clean = df_teacher_raw.copy()\n",
    "\n",
    "# Apply mappings\n",
    "decoded_count = 0\n",
    "for col in df_teacher_clean.columns:\n",
    "    if col in teacher_mappings:\n",
    "        print(f\"  Decoding: {col}\")\n",
    "        df_teacher_clean[col] = df_teacher_clean[col].map(teacher_mappings[col]).fillna(df_teacher_clean[col])\n",
    "        decoded_count += 1\n",
    "\n",
    "print(f\"\\n‚úì Decoded {decoded_count} columns in teacher data\")\n",
    "\n",
    "# Add metadata columns\n",
    "df_teacher_clean['User_Type'] = 'Teacher'\n",
    "df_teacher_clean['Dataset_Source'] = 'Brazilian_Teacher_Burnout_COVID19'\n",
    "\n",
    "print(\"\\n--- TEACHER DATA SAMPLE (DECODED) ---\")\n",
    "# Show first few columns that likely got decoded\n",
    "display(df_teacher_clean.iloc[:, :8].head(5))\n",
    "\n",
    "print(f\"\\nFinal Teacher Data Shape: {df_teacher_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 3C: INSPECT COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 3C: INSPECT COLUMNS FOR STANDARDIZATION\n",
    "# ============================================\n",
    "\n",
    "print(\"STUDENT COLUMNS:\")\n",
    "print(\"=\"*80)\n",
    "for i, col in enumerate(df_student_clean.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "print(\"\\n\\nTEACHER COLUMNS:\")\n",
    "print(\"=\"*80)\n",
    "for i, col in enumerate(df_teacher_clean.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ACTION REQUIRED:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Review the columns above and identify:\")\n",
    "print(\"1. Which student columns correspond to which teacher columns?\")\n",
    "print(\"2. Which columns contain demographic info (age, gender, etc.)?\")\n",
    "print(\"3. Which columns contain outcomes (burnout scores, satisfaction, etc.)?\")\n",
    "print(\"\\nWe'll use this information in the next cell to create standardized names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 4: DATA INTEGRATION & STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 4: DATA INTEGRATION & STANDARDIZATION\n",
    "# ============================================\n",
    "\n",
    "# ========== STEP 1: RENAME COLUMNS ==========\n",
    "# Based on the actual columns, create meaningful standardized names\n",
    "# IMPORTANT: Update these mappings based on what you saw in Cell 5.5\n",
    "\n",
    "# Student Survey - Based on Q-codes, map to meaningful names\n",
    "student_renames = {\n",
    "    'Q1': 'Country_Study',\n",
    "    'Q3': 'Is_Citizen',\n",
    "    'Q4': 'Student_Status',  # Full-time/Part-time\n",
    "    'Q5': 'Study_Level',     # Bachelor/Master/Doctoral\n",
    "    'Q6': 'Field_of_Study',\n",
    "    'Q7': 'Age',\n",
    "    'Q8': 'Gender',\n",
    "    'Q9': 'Had_Onsite_Classes',\n",
    "    # Add more Q-codes as you identify them from Cell 5.5\n",
    "    # Example: 'Q10a': 'Satisfaction_RealTime_Video', etc.\n",
    "}\n",
    "\n",
    "# Teacher Burnout - Based on visible columns\n",
    "teacher_renames = {\n",
    "    'Age:': 'Age',\n",
    "    'Sex:': 'Gender',\n",
    "    'Marital status:': 'Marital_Status',\n",
    "    'I live with:': 'Living_Situation',\n",
    "    'Area of knowledge in which it operates:': 'Knowledge_Area',\n",
    "    'Level of training (titration)': 'Education_Level',\n",
    "    'Main level of education in which it operates:': 'Teaching_Level',\n",
    "    # Add burnout-related columns here as you identify them\n",
    "}\n",
    "\n",
    "# Apply renames (only rename columns that exist)\n",
    "student_renames_filtered = {k: v for k, v in student_renames.items() if k in df_student_clean.columns}\n",
    "teacher_renames_filtered = {k: v for k, v in teacher_renames.items() if k in df_teacher_clean.columns}\n",
    "\n",
    "df_student_standardized = df_student_clean.rename(columns=student_renames_filtered)\n",
    "df_teacher_standardized = df_teacher_clean.rename(columns=teacher_renames_filtered)\n",
    "\n",
    "print(f\"‚úì Column names standardized\")\n",
    "print(f\"  Student: {len(student_renames_filtered)} columns renamed\")\n",
    "print(f\"  Teacher: {len(teacher_renames_filtered)} columns renamed\")\n",
    "\n",
    "\n",
    "# ========== STEP 2: CREATE AGE GROUPS ==========\n",
    "def create_age_groups(age_value):\n",
    "    \"\"\"Converts age to standard groups for comparison\"\"\"\n",
    "    try:\n",
    "        age = int(float(age_value))\n",
    "        if age < 25:\n",
    "            return '18-24'\n",
    "        elif age < 35:\n",
    "            return '25-34'\n",
    "        elif age < 45:\n",
    "            return '35-44'\n",
    "        elif age < 55:\n",
    "            return '45-54'\n",
    "        else:\n",
    "            return '55+'\n",
    "    except (ValueError, TypeError):\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply age grouping if Age column exists\n",
    "if 'Age' in df_student_standardized.columns:\n",
    "    df_student_standardized['Age_Group'] = df_student_standardized['Age'].apply(create_age_groups)\n",
    "    df_student_standardized['Age_Numeric'] = pd.to_numeric(df_student_standardized['Age'], errors='coerce')\n",
    "    print(\"‚úì Student age groups created\")\n",
    "\n",
    "if 'Age' in df_teacher_standardized.columns:\n",
    "    df_teacher_standardized['Age_Group'] = df_teacher_standardized['Age'].apply(create_age_groups)\n",
    "    df_teacher_standardized['Age_Numeric'] = pd.to_numeric(df_teacher_standardized['Age'], errors='coerce')\n",
    "    print(\"‚úì Teacher age groups created\")\n",
    "\n",
    "\n",
    "# ========== STEP 3: STANDARDIZE GENDER ==========\n",
    "def standardize_gender(gender_value):\n",
    "    \"\"\"Standardizes gender values\"\"\"\n",
    "    if pd.isna(gender_value):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    gender_str = str(gender_value).lower().strip()\n",
    "    \n",
    "    if 'male' in gender_str and 'female' not in gender_str:\n",
    "        return 'Male'\n",
    "    elif 'female' in gender_str:\n",
    "        return 'Female'\n",
    "    elif 'diverse' in gender_str or 'other' in gender_str:\n",
    "        return 'Gender Diverse'\n",
    "    elif 'prefer not' in gender_str:\n",
    "        return 'Prefer Not to Say'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "if 'Gender' in df_student_standardized.columns:\n",
    "    df_student_standardized['Gender_Standardized'] = df_student_standardized['Gender'].apply(standardize_gender)\n",
    "    print(\"‚úì Student gender standardized\")\n",
    "\n",
    "if 'Gender' in df_teacher_standardized.columns:\n",
    "    df_teacher_standardized['Gender_Standardized'] = df_teacher_standardized['Gender'].apply(standardize_gender)\n",
    "    print(\"‚úì Teacher gender standardized\")\n",
    "\n",
    "\n",
    "# ========== STEP 4: CREATE FINAL DATASETS ==========\n",
    "df_student_final = df_student_standardized.copy()\n",
    "df_teacher_final = df_teacher_standardized.copy()\n",
    "\n",
    "print(f\"\\n‚úì Standardization complete\")\n",
    "print(f\"  Student final shape: {df_student_final.shape}\")\n",
    "print(f\"  Teacher final shape: {df_teacher_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 4.5: DATA QUALITY VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 4.5: DATA QUALITY VALIDATION\n",
    "# ============================================\n",
    "\n",
    "def quality_report(df, dataset_name):\n",
    "    \"\"\"Comprehensive data quality report\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATA QUALITY REPORT: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nüìä BASIC INFO:\")\n",
    "    print(f\"  Total Records: {len(df):,}\")\n",
    "    print(f\"  Total Columns: {len(df.columns)}\")\n",
    "    print(f\"  Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n‚ùå MISSING VALUES:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing,\n",
    "        'Missing_Percent': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df.head(10))\n",
    "    else:\n",
    "        print(\"  ‚úì No missing values!\")\n",
    "    \n",
    "    print(f\"\\nüîÅ DUPLICATES:\")\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f\"  Duplicate rows: {dup_count}\")\n",
    "    \n",
    "    print(f\"\\nüìà DATA TYPES:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(f\"\\nüéØ KEY COLUMNS CHECK:\")\n",
    "    key_cols = ['Age', 'Gender', 'User_Type', 'Age_Group', 'Gender_Standardized']\n",
    "    for col in key_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"  {col}: {df[col].nunique()} unique values\")\n",
    "            value_counts = df[col].value_counts().head(3).to_dict()\n",
    "            print(f\"    Sample: {value_counts}\")\n",
    "\n",
    "# Run reports\n",
    "quality_report(df_student_final, \"STUDENTS (Final)\")\n",
    "quality_report(df_teacher_final, \"TEACHERS (Final)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 5: EXPORT TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 5: EXPORT TO CSV\n",
    "# ============================================\n",
    "\n",
    "# Define output paths\n",
    "student_out_path = os.path.join(PROCESSED_DATA_DIR, 'students_cleaned.csv')\n",
    "teacher_out_path = os.path.join(PROCESSED_DATA_DIR, 'teachers_cleaned.csv')\n",
    "\n",
    "# Export individual datasets\n",
    "df_student_final.to_csv(student_out_path, index=False, encoding='utf-8-sig')\n",
    "df_teacher_final.to_csv(teacher_out_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"‚úì Successfully exported individual datasets:\")\n",
    "print(f\"  1. {student_out_path}\")\n",
    "print(f\"  2. {teacher_out_path}\")\n",
    "\n",
    "\n",
    "# ========== CREATE UNIFIED DATASET ==========\n",
    "# For BI tools, create a harmonized schema with common dimensions\n",
    "\n",
    "# Define core dimensions that exist in both (or can be added)\n",
    "core_dimensions = [\n",
    "    'User_Type',\n",
    "    'Dataset_Source',\n",
    "    'Age',\n",
    "    'Age_Group',\n",
    "    'Age_Numeric',\n",
    "    'Gender',\n",
    "    'Gender_Standardized',\n",
    "]\n",
    "\n",
    "# Add these columns to both if missing\n",
    "for col in core_dimensions:\n",
    "    if col not in df_student_final.columns:\n",
    "        df_student_final[col] = None\n",
    "    if col not in df_teacher_final.columns:\n",
    "        df_teacher_final[col] = None\n",
    "\n",
    "# Select only core columns for combined dataset\n",
    "df_combined = pd.concat([\n",
    "    df_student_final[core_dimensions],\n",
    "    df_teacher_final[core_dimensions]\n",
    "], ignore_index=True)\n",
    "\n",
    "combined_out_path = os.path.join(PROCESSED_DATA_DIR, 'combined_demographics.csv')\n",
    "df_combined.to_csv(combined_out_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"  3. {combined_out_path} (Common demographics only)\")\n",
    "\n",
    "\n",
    "# ========== CREATE DATA DICTIONARY ==========\n",
    "def create_data_dictionary(df, dataset_name):\n",
    "    \"\"\"Creates metadata about the dataset\"\"\"\n",
    "    data_dict = pd.DataFrame({\n",
    "        'Column_Name': df.columns,\n",
    "        'Data_Type': df.dtypes.astype(str),\n",
    "        'Non_Null_Count': df.count(),\n",
    "        'Null_Count': df.isnull().sum(),\n",
    "        'Unique_Values': [df[col].nunique() for col in df.columns],\n",
    "        'Sample_Value_1': [df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else None for col in df.columns],\n",
    "        'Sample_Value_2': [df[col].dropna().iloc[1] if len(df[col].dropna()) > 1 else None for col in df.columns],\n",
    "    })\n",
    "    \n",
    "    dict_path = os.path.join(PROCESSED_DATA_DIR, f'data_dictionary_{dataset_name}.csv')\n",
    "    data_dict.to_csv(dict_path, index=False)\n",
    "    return dict_path\n",
    "\n",
    "dict_student = create_data_dictionary(df_student_final, 'students')\n",
    "dict_teacher = create_data_dictionary(df_teacher_final, 'teachers')\n",
    "\n",
    "print(f\"  4. {dict_student}\")\n",
    "print(f\"  5. {dict_teacher}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PHASE 1-5 COMPLETE - DATA READY FOR BI VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProcessed files location: {PROCESSED_DATA_DIR}\")\n",
    "print(\"\\nYou can now proceed to Phase 6 for analysis and visualization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
